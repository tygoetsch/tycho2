\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Kris Garrett}
\title{Tycho 2 User Guide}

\hypersetup{colorlinks,urlcolor=blue}


\begin{document}
\maketitle


\section{Introduction}
Tycho 2 is a reworking of a code named Tycho written by Shawn Pautz around the year 2000.
The original code solved a linear, kinetic, transport equation on an unstructured, 3D tetrahedral mesh and was parallelized via MPI.
The original code was made to test the performance of transport sweeps which will be subsequently described.

Tycho 2 solves the same equation with some additions.
The new code adds energy dependence to the calculations to give realistic computational results for cases with several energy groups.
The code is implemented in MPI and OpenMP and uses only open source software and libraries to generate meshes and compile.







\section{Compiling and running the code}
The code is split into the main executable named {\tt sweep.x} and several utilities dealing with meshes in the {\tt util} directory.
Compilation of all files requires a C++11 compliant compiler, MPI, and OpenMP.
Some of the utilities require extra libraries, which will be detailed.


\paragraph{General Workflow}
To build and run Tycho 2, follow this general workflow.
\begin{itemize}
\item In the top directory, copy {\tt make.inc.example} to {\tt make.inc}.  Amend the variables in {\tt make.inc}.  Then run {\tt make} to build the main program {\tt sweep.x}.
\item In the {\tt util} directory, copy {\tt make.inc.example} to {\tt make.inc}.  Amend the variables in {\tt make.inc}.  Then run {\tt make <utility name>} to build the associated utility.  Each {\tt .cc} file in {\tt util} represents a utility name.  For example {\tt make PartitionColumns} will compile {\tt PartitionColumns.cc} and create the binary {\tt PartitionColumns.x}.  Running {\tt make} will build all the utilities which is probably not something you want to do.
\item Build a utility in {\tt util} to partition a serial mesh (smesh) into a parallel mesh (pmesh).  Use either {\tt PartitionColumns} or {\tt SerialToParallelMesh}.
\item In the top directory, copy {\tt input.deck.example} to {\tt input.deck} and amend parameters.
\item In the top directory, copy {\tt run.sh.example} to {\tt run.sh} and amend.
\item Execute the script {\tt run.sh}.
\end{itemize}


\subsection{The util directory}
The {\tt util} directory contains several utilities for creating and partitioning meshes that {\tt sweep.x} will use.
When you clone Tycho 2 from github, you should see the following files in the {\tt util} directory.
\begin{itemize}
\item {\tt cube\_no\_mesh.hdf} -- File containing a cube's geometry.  The cube is a 100x100x100 cube.  This can be opened by certain CAD programs to be meshed such as Salome.\\
\url{http://www.salome-platform.org/}
\item {\tt cube-<N>.cgns} -- An unstructured, tetrahedral mesh of {\tt cube\_no\_mesh.hdf} in the CGNS file format where $N$ is the number of cells in the mesh. \\
\url{http://cgns.github.io/}
\item {\tt cube-<N>.smesh} -- The same meshes as given by {\tt cube-<N>.cgns}, just in a file format made for Tycho 2 that does not require any library dependencies to be read.
\item {\tt CreateGaussLegendre.py} -- This is a Python script used to create the Gauss Legendre quadrature in the main code at {\tt src/Quadrature.cc}.
\item {\tt CreateRegularMesh.cc} -- Creates a regular Cartesian mesh and then each rectangular prism is split into tetrahedra. \\
{\color{red} No dependencies required.}
\item {\tt MoabToSerialMesh.cc} -- Uses the MOAB mesh library to read in a common mesh format and convert it to the smesh file format. \\
\url{http://sigma.mcs.anl.gov/moab-library/} \\
{\color{red} Requires MOAB at the least.  May also require CGNS, HDF5, NetCDF, etc for your input mesh format.}
\item {\tt ParallelMeshInfo.cc} -- Writes data about a pmesh.\\
{\color{red} No dependencies required.}
\item {\tt ParallelMeshToMoab.cc} -- Converts the pmesh file format into a file format that the Visit visualization tool can read.\\
{\color{red} Requires MOAB.}
\item {\tt PartitionColumns.cc} -- Partitions an smesh into a pmesh by columns.  This requires you to specify two numbers: the number of partitions in the x-direction and the number of partitions in the y-direction. \\
{\color{red} No dependencies required.}
\item {\tt RefineSerialMesh.cc} -- Creates a new smesh that has been refined by 8x.\\
{\color{red} No dependencies required.}
\item {\tt SerialMeshInfo.cc} -- Writes data about an smesh.\\
{\color{red} No dependencies required.}
\item {\tt SerialMeshToMoab.cc} -- Converts the smesh file format into a file format that the Visit visualization tool can read.\\
{\color{red} Requires MOAB.}
\item {\tt SerialToParallelMesh.cc} -- Partitions an smesh into a pmesh using the Metis partitioning library.\\
\url{http://glaros.dtc.umn.edu/gkhome/metis/metis/overview} \\
{\color{red} Requires Metis (just the serial version).}
\end{itemize}


\paragraph{Creating a pmesh}
The one utility you will need to compile is a utility to partition an smesh into a pmesh.
For this, either use {\tt PartitionColumns} or {\tt SerialToParallelMesh}.
The first partitions a 3D domain into columns in the z-axis.
The second partitions a 3D domain using Metis, which attempts to minimize surface area between domains.


\paragraph{Scaling}
If you want to do a weak scaling study, you will need the {\tt RefineSerialMesh} utility to create a series of meshes that are 8x larger than one another.
If you want to visualize your mesh, you will need the {\tt SerialMeshToMoab} or {\tt ParallelMeshToMoab} utilities.
The other utilities are mainly for development purposes.


\paragraph{Visualization}
If you wish to visualize either an smesh or pmesh, you will use the {\tt SerialMeshToMoab} or {\tt ParallelMeshToMoab} utility.
These require the MOAB mesh library, but no other dependencies are required.
SerialMeshToMoab will create a {\tt .vtk} file that can be read by Visit.
ParallelMeshToMoab will create a {\tt .vtk} file for each partition of the pmesh and a {\tt .visit} file linking all the {\tt .vtk} files together.
You can then read the {\tt .vtk} or {\tt .visit} file in Visit to visualize the mesh and its partitions.

To use Visit, press the {\tt Open} button on the side panel to select the {\tt .vtk} or {\tt .visit} file you wish to visualize.
Then in the same panel select {\tt Add->mesh->mesh} and if visualizing the partitions select {\tt Add->subset->domains}.
Then click {\tt Draw} to view the mesh.


\paragraph{Creating your own mesh}
You probably don't want to do this, but here are instructions for converting your own prebuilt mesh into a mesh format that Tycho 2 can read.
You will need the {\tt MoabToSerialMesh} utility which requires the MOAB mesh library.
MOAB can read several mesh file formats, but extra libraries may have to be included in the build process for your mesh format.
Currently, the initial condition for Tycho 2 assumes a domain with dimensions inside the cube $(0,0,0)$ and $(100,100,100)$.
If your mesh does not sit inside this cube, you will have an initial condition of zero.





\subsection{Running sweep.x}
The main executable is parallelized via MPI and OpenMP.
To set the number of OpenMP threads, set the {\tt OMP\_NUM\_THREADS} environment variable.
To run {\tt sweep.x}, type: {\tt mpirun -n <\# procs> ./sweep.x <.pmesh file> <input.deck file>}.
This requires that you have created a pmesh as described in the previous section and you have copied {\tt input.deck.example} to {\tt input.deck} (or any other name you prefer).

\paragraph{input.deck options}
\begin{itemize}
\item {\tt snOrder} -- Order of the angular quadrature.  This can be any even number from 2 to 30.
\item {\tt iterMax} -- Maximum number of iterations for source iteration
\item {\tt errMax} -- Tolerance for the relative error for source iteration
\item {\tt maxCellsPerStep} -- Maximum number of of cell/angle pairs to compute for $\Psi$ before communication via MPI
\item {\tt intraAngleP} -- This can be 0, 1, 2, 3, or 4 for random, b-level, BFDS, DFDS, and DFHDS
\item {\tt interAngleP} -- This can be 0, 1, or 2 for interleaved, globally prioritized, and locally prioritized
\item {\tt nGroups} -- Number of energy group.  Should be 1 or greater.
\item {\tt sigmaTotal} -- Total cross section.
\item {\tt sigmaScat} -- Scattering cross section.
\item {\tt OutputFile} -- Boolean indicating whether to write out final result of $\Psi$.
\item {\tt OutputFilename} -- Output file name if {\tt OutputFile = true}.
\item {\tt DD\_IterMax} -- Maximum number of iterations for domain decomposition methods.
\item {\tt DD\_ErrMax} -- Tolerance for the relative error of domain decomposition methods.
\item {\tt SweepType} Type of sweeper to use.  Possible values are commented in the {\tt input.deck.example} file.
\item {\tt GaussElim} -- Type of solver to use for the within cell DG systems as given by Equation~\eqref{eq:dg_system}.
\end{itemize}





\section{The underlying mathematics}
\subsection{The equation}
Tycho 2 solves the following kinetic equation with isotropic scattering
\begin{equation} \label{eq:main}
\Omega \cdot \nabla_x \Psi(x,\Omega,E) + \sigma_t \Psi(x,\Omega,E) = \frac{\sigma_s}{4\pi} \int_{\mathbb{S}^2} \Psi(x,\Omega',E) d\Omega' + Q(x,\Omega,E).
\end{equation}
The function $\Psi$ is the unknown, $\sigma_t$ and $\sigma_s$ are the total and scattering cross sections with $\sigma_t > \sigma_s$ and are constant.
The function $Q$ is a known source.

The independent variables are:
\begin{itemize}
\item Space -- $x \in \mathcal{D} \subset \mathbb{R}^3$,
\item Direction -- $\Omega \in \mathbb{S}^2$ (the unit sphere), 
\item Energy -- $E \in \mathbb{R}^{\geq 0}$.
\end{itemize}
Notice the equation is dependent on energy, but the different energies are not coupled.
This is done on purpose since the goal is to test sweeping strategies which are uncoupled in energy.

For the purpose of simpler notation, define
\begin{equation}
\Phi(x,E) = \int_{\mathbb{S}^2} \Psi(x,\Omega',E) d\Omega'
\end{equation}
to get the equivalent equation
\begin{equation} \label{eq:main1}
\Omega \cdot \nabla_x \Psi(x,\Omega,E) + \sigma_t \Psi(x,\Omega,E) = \frac{\sigma_s}{4\pi} \Phi(x,E) + Q(x,\Omega,E).
\end{equation}


\subsection{Method of discretization}
Equation \eqref{eq:main1} is discretized using: discontinuous Galerkin (DG) with linear elements in $x$, discrete ordinates in $\Omega$, and energy groups in $E$.


\paragraph{Energy discretization.}
Since equation~\eqref{eq:main1} is not coupled in energy, the method of discretization of $E$ does not matter.
However, it is typical to discretize $E$ into energy groups.
Hence, the discretization in energy is denoted by $E_g$ where $g$ is an integer indexing the energy group (though any other discretization is fine).
\begin{equation}
\Omega \cdot \nabla_x \Psi_g(x,\Omega) + \sigma_t \Psi_g(x,\Omega) = \frac{\sigma_s}{4\pi} \int_{\mathbb{S}^2} \Psi_g(x,\Omega') d\Omega' + Q_g(x,\Omega).
\end{equation}


\paragraph{Angle discretization.}
Angle is discretized via discrete ordinates, which is often denoted as S$_N$ where $N$ is the order of the discretization.
Tycho 2 implements the Chebyshev-Legendre quadrature of the sphere.
This involves a Cartesian product of $N$ Legendre nodes on the z-axis and $2N$ equally spaced points on the circle for each Legendre node.
The weights are proportional to the weights of the Legendre quadrature and scaled to sum to $4\pi$.
The nodes and weights will be denoted by $\Omega_q$ and $w_q$, where $q$ stands for the quadrature index.

Equation~\eqref{eq:main} becomes
\begin{equation}
\Omega_q \cdot \nabla_x \Psi_{qg}(x) + \sigma_t \Psi_{qg}(x) = 
\frac{\sigma_s}{4\pi} \sum_{q'=0}^{2N^2-1} w_{q'} \Psi_{q'g}(x) + Q_{qg}(x).
\end{equation}
Equation~\eqref{eq:main1} becomes
\begin{equation}
\Omega_q \cdot \nabla_x \Psi_{qg}(x) + \sigma_t \Psi_{qg}(x) = 
\frac{\sigma_s}{4\pi} \Phi_{g}(x) + Q_{qg}(x),
\end{equation}
where
\begin{equation}
\Phi_g(x) = \sum_{q'=0}^{2N^2-1} w_{q'} \Psi_{q'g}(x).
\end{equation}
Below is a depiction of the quadrature nodes on the sphere.

\includegraphics[scale=0.5]{chebyshev-legendre.jpg}


\paragraph{Spatial discretization.}
Since source iteration is used to converge the RHS of equation~\eqref{eq:main}, it will be easier to show the equations for the simpler problem
\begin{equation}
\Omega_q \cdot \nabla_x \Psi_{qg}(x) + \sigma_t \Psi_{qg}(x) = Q_{qg}(x).
\end{equation}
In this formulation, the scattering term has been absorbed into the source $Q_{qg}(x)$.

Space is discretized via linear DG.
In each tetrahedral cell $C_i$, define basis functions $b_{ik}|_{k=0}^3$ to be linear interpolation functions with 
\begin{equation}
b_{ik} = \begin{cases}
1, \textrm{ at vertex } k \textrm{ of cell } C_i \\
0, \textrm{ at other vertices of cell } C_i
\end{cases}.
\end{equation}
Also, define 
\begin{equation}
\Psi^h|_{C_i} = \sum_k \Psi_{ik} b_k.
\end{equation}

Then the DG formulation is derived by starting with the equation
\begin{equation} \label{eq:sweep}
\Omega \cdot \nabla_x \Psi(x) + \sigma_t \Psi(x) = Q(x),
\end{equation}
where the subscripts $q,g$ are implicit.
Then multiply by test function $b_j$ in cell $C_i$ and integrate to get
\begin{equation}
\int_{C_i} \Omega \cdot \nabla_x \Psi b_j dx + \int_{C_i} \sigma_t \Psi b_j dx = \int_{C_i} Q b_j dx.
\end{equation}
Then use integration by parts on the first integral to get
\begin{equation}
-\int_{C_i} \Omega \cdot \nabla_x b_j \Psi dx + 
\sum_k \int_{F_{ik}} (\Omega \cdot \nu_k) b_j \hat{\Psi}^{(k)} dA + 
\int_{C_i} \sigma_t \Psi b_j dx = \int_{C_i} Q b_j dx.
\end{equation}
Here, $k$ indexes over the faces $F_{ik}$ of cell $i$, and $\nu_k$ is the outward normal to face $F_{ik}$.
The only thing left to define is the numerical flux $\hat{\Psi}^{(k)}$.
Since $\Psi$ is discontinuous at the faces, this is not well defined and must be defined by the numerical method.
We use the upwind flux
\begin{equation}
\hat{\Psi}^{(k)} = \begin{cases}
\Psi|_{F_{ik}} \textrm{ from } C_{i}, & \textrm{if } \Omega \cdot \nu_k > 0 \\
\Psi|_{F_{ik}} \textrm{ from adjacent cell} , & \textrm{if } \Omega \cdot \nu_k < 0 \\
\end{cases}
\end{equation}

Putting everything together, one gets the linear system below.
The derivation is described in an accompanying document.
Face $k$ is defined to be opposite vertex $k$ as shown in the figure.
\begin{subequations} \label{eq:dg_system}
\begin{align}
\frac{1}{12} \bar{A}_1 \left( 2\hat{\Psi}^{(1)}_0 + \hat{\Psi}^{(1)}_2 + \hat{\Psi}^{(1)}_3 \right)
+ \frac{1}{12} \bar{A}_2 \left( 2\hat{\Psi}^{(2)}_0 + \hat{\Psi}^{(2)}_1 + \hat{\Psi}^{(2)}_3 \right)
+ \frac{1}{12} \bar{A}_3 \left( 2\hat{\Psi}^{(3)}_0 + \hat{\Psi}^{(3)}_1 + \hat{\Psi}^{(3)}_2 \right) \notag \\
+ \frac{1}{12} \bar{A}_0 \left( \Psi_0 + \Psi_1 + \Psi_2 + \Psi_3 \right)
+ \frac{\sigma_t V}{20} \left( 2\Psi_0 + \Psi_1 + \Psi_2 + \Psi_3 \right)
= \frac{V}{20} \left( 2Q_0 + Q_1 + Q_2 + Q_3 \right)
\end{align}
\begin{align}
\frac{1}{12} \bar{A}_0 \left( 2\hat{\Psi}^{(0)}_1 + \hat{\Psi}^{(0)}_2 + \hat{\Psi}^{(0)}_3 \right)
+ \frac{1}{12} \bar{A}_2 \left( \hat{\Psi}^{(2)}_0 + 2\hat{\Psi}^{(2)}_1 + \hat{\Psi}^{(2)}_3 \right)
+ \frac{1}{12} \bar{A}_3 \left( \hat{\Psi}^{(3)}_0 + 2\hat{\Psi}^{(3)}_1 + \hat{\Psi}^{(3)}_2 \right) \notag \\
+ \frac{1}{12} \bar{A}_1 \left( \Psi_0 + \Psi_1 + \Psi_2 + \Psi_3 \right)
+ \frac{\sigma_t V}{20} \left( \Psi_0 + 2\Psi_1 + \Psi_2 + \Psi_3 \right)
= \frac{V}{20} \left( Q_0 + 2Q_1 + Q_2 + Q_3 \right)
\end{align}
\begin{align}
\frac{1}{12} \bar{A}_0 \left( \hat{\Psi}^{(0)}_1 + 2\hat{\Psi}^{(0)}_2 + \hat{\Psi}^{(0)}_3 \right)
+ \frac{1}{12} \bar{A}_1 \left( \hat{\Psi}^{(1)}_0 + 2\hat{\Psi}^{(1)}_2 + \hat{\Psi}^{(1)}_3 \right)
+ \frac{1}{12} \bar{A}_3 \left( \hat{\Psi}^{(3)}_0 + \hat{\Psi}^{(3)}_1 + 2\hat{\Psi}^{(3)}_2 \right) \notag \\
+ \frac{1}{12} \bar{A}_2 \left( \Psi_0 + \Psi_1 + \Psi_2 + \Psi_3 \right)
+ \frac{\sigma_t V}{20} \left( \Psi_0 + \Psi_1 + 2\Psi_2 + \Psi_3 \right)
= \frac{V}{20} \left( Q_0 + Q_1 + 2Q_2 + Q_3 \right)
\end{align}
\begin{align}
\frac{1}{12} \bar{A}_0 \left( \hat{\Psi}^{(0)}_1 + \hat{\Psi}^{(0)}_2 + 2\hat{\Psi}^{(0)}_3 \right)
+ \frac{1}{12} \bar{A}_1 \left( \hat{\Psi}^{(1)}_0 + \hat{\Psi}^{(1)}_2 + 2\hat{\Psi}^{(1)}_3 \right)
+ \frac{1}{12} \bar{A}_2 \left( \hat{\Psi}^{(2)}_0 + \hat{\Psi}^{(2)}_1 + 2\hat{\Psi}^{(2)}_3 \right) \notag \\
+ \frac{1}{12} \bar{A}_3 \left( \Psi_0 + \Psi_1 + \Psi_2 + \Psi_3 \right)
+ \frac{\sigma_t V}{20} \left( \Psi_0 + \Psi_1 + \Psi_2 + 2\Psi_3 \right)
= \frac{V}{20} \left( Q_0 + Q_1 + Q_2 + 2Q_3 \right)
\end{align}
\end{subequations}


\includegraphics[scale=0.5]{tet-eps-converted-to.pdf}





\subsection{Source iteration}
The overall algorithm is to lag the data from the integral on the RHS of equation~\eqref{eq:main1}.
This is called source iteration
\begin{equation}
\Omega \cdot \nabla_x \Psi^{k+1} + \sigma_t \Psi^{k+1} = \frac{\sigma_s}{4\pi} \Phi^k + Q.
\end{equation}
Tycho 2 iterates this process until $||\Phi^{k+1} - \Phi^k||_\infty / ||\Phi^{k+1}||_\infty < \textrm{tolerance}$.



\subsection{Sweeps}
The most common method of solving the discretized form of equation~\eqref{eq:sweep} for $\Psi$ is called a sweep.
Consider the following 2D example shown below.

\includegraphics[scale=0.75]{grid2d.pdf}
\includegraphics[scale=0.75]{dag.pdf}

The sweep algorithm for the transport direction (angle) shown, shows that cells 1 and 10 have no dependencies except for the incoming boundary.
Once $\Psi$ is computed in these cells, the upwind flux is calculated for the boundaries between (1,2), (1,6), and (10,11).
With this boundary data between cells known, $\Psi$ in cells 2, 6, and 11 can be computed.
This continues until $\Psi$ is computed in all cells.

A sweep is needed for each angle and energy group.
Fortunately, all these sweeps are completely independent and therefore trivially parallelizable.

\subsubsection{Sweeps Algorithms}
Several sweep algorithms have been employed.
The input deck currently allows 4 types of sweeps: {\tt OriginalTycho1}, {\tt OriginalTycho2}, {\tt GraphTraversal}, and {\tt PBJ}.
All but the {\tt PBJ} algorithm rely on each cell-angle pair having a priority assigned.
To understand why, consider the same sweep problem as shown in the previous figure but now split via MPI into 2 partitions.

\includegraphics[scale=0.75]{dag_partitioned.pdf}

For partition 2, cell 10 must be computed first, then cell 11.
After this, there is a choice.
Either cell 12 or 19 can be computed, but cell 12 is probably best so that data can be communicated to compute cell 13 in partition 1.
Generally, the cell-angle priorities will attempt to get data to a partition boundary as quickly as possible.
The formulas for the priorities are detailed below but an explanation of the reasoning for each priority may be found in \cite{pautz-2002}.

\subsubsection{Intra-Angle Priorities}
Priorities are first created cells in each angle separately.
This is the definition of the intra-angle priorities.
Fundamental to all the priority calculations is the notion of B-Level of a cell.
This is defined to be the longest distance to leaf of the {\em entire} graph (not just for the partition).
For instance, node 17 has a B-Level of 1, node 16 has a B-Level of 0, and node 10 has a B-Level of 6 by traversing cells: 10, 11, 12, 13, 14, 15, 16.
Leaves of a graph have a B-Level of zero.

\paragraph{Random}
The random priority assigns a uniformly random priority to all cells.

\paragraph{B-Level}
The b-level priority assigns to each cell a priority equal to the cell's b-level.

\paragraph{BFDS}
This stands for Breadth First Descendant Seeking.
Each cell's priority is equal to the highest b-level of any descendant in another partition.
If there is no descendant in another partition, the priority is set to zero.

\paragraph{DFDS}
This stands for Depth First Descendant Seeking.
Each cell with a child in another partition has a priority equal to the maximum b-level of such children plus a constant equal or greater than the number of levels of the graph.
Parents are recursively assigned a priority of one less than the maximum of its children.
If a cell has no descendants in another partition, its priority is set to zero.

\paragraph{DFHDS}
This stands for Depth First Highest Descendant Seeking.
Each cell with a child in another partition has a priority equal to the maximum b-level of such children multiplied by (rather than addition as in DFDS) a constant equal or greater than the number of levels of the graph.
The rest of the algorithm is the same as for DFDS.

\subsubsection{Inter-Angle Priorities}
For a given cell, a priority is assigned for each angle independently via one of the intra-angle priorities above.
Now these priorities must be combined.

\paragraph{Interleaved}
For this priority type, the inter-angle priority is set to the intra-angle priority.
This can cause angles to be interleaved when cell-angle pairs are sorted by priority.

\paragraph{Globally Prioritized}
A sufficiently large constant is multiplied by the angle index to ensure a cell-angle pair with angle index less than another angle index has a lower priority.

\paragraph{Locally Prioritized}
First, for each angle, the maximum priority across cells is calculated to created an array of maximum priorities (with size of the number of angles).
The max priority array is sorted and angle indices are reordered with respect to the sorted max priority array.
Then the globally prioritized algorithm is used.


\subsubsection{The Sweep Algorithms}

The general sweep algorithm works as follows.
\begin{verbatim}
Assign priorities for each cell-angle pair
Calculate number of dependencies (parents) for each cell-angle pair
Place all cell-angle pairs with no dependencies in a priority queue
While (Still some cell-angle pair to compute)
  For i = 1 to maxCellsPerStep
    Pop cell-angle pair off priority queue
    Calculate Psi on cell-angle pair
    Update number of dependencies for children on MPI rank
    If (child has no dependencies)
      Put child in priority queue
    Send/Recv MPI rank's boundary data
    Update dependencies
    Place any cell-angle pairs with no dependencies in priority queue
\end{verbatim}


\paragraph{OriginalTycho 1 and 2}
Both of these sweep types use the same overall algorithm but differ on their OpenMP implementation.
In this section, the OpenMP implementation will not be discussed, and the text below equally describes both sweep types.

For this sweep type, the schedule of updates for $\Psi$ on each cell-angle pair is precomputed.
The for loop in the algorithm above is called a step and a queue of computational tasks is initially created for that loop.
Then, when it comes time to do the sweep, tasks are popped off the queue for each step.

\paragraph{GraphTraversal}
This sweep algorithm follows the above algorithm exactly.
In particular, the dependencies are updated calculated in real time for the sweep as opposed to precomputing them like the OriginalTycho algorithms.


\paragraph{PBJ}
PBJ stands for Parallel Block Jacobi.
In this algorithm, every MPI rank has an initial guess for the boundary data and a sweep is performed for each partition without regard for neighboring partitions.
Then boundary data is communicated between all MPI ranks and sweeps are performed on each individual partition again.
This is iterated until $\Psi$ does not change.



\begin{thebibliography}{1}

  \bibitem{pautz-2002} Shawn D. Pautz, ``An Algorithm for Parallel S$_n$ Sweeps on Unstructed Meshes,'' {\em Nuclear Science and Engineering}, 140, 111-136 (2002).

  

\end{thebibliography}













\end{document}